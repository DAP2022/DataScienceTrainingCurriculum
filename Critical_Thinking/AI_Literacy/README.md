# AI Literacy & DS Ethics

** This is a 12-hour mini-course that is part of the Jackson Laboratory Foundational Data Science Training Curriculum ** 
** The course content represented below is from the December 2025 version of the course. The content order will shift around a bit as we incorporate different guest researcher speakers each iteration **

## Course Summary
Ai literacy is emancipatory. Understanding the limits and benefits of popular Ai tools is a requirement for individuals to autonomously participate in all aspects of modern society. This 12-hour course is a **code-free** exploration of Ai tools, with an emphasis on broadly understanding - and interacting with - different types of Machine Learning, and Large Language Models. Along with an introduction to the concepts central to Ai, we will also hear JAX speakers explain how they are practically incorporating Ai/LLM to optimize their own research.

## Course Learning Outcomes  
1. Day 1 – Foundations of AI, ML & Data Science
-	Distinguish “AI”, “machine learning” and “data-science / statistics” and recognize where supervised vs. unsupervised learning fit in the conceptual network.
-	Trace key milestones in the history of AI to appreciate that the field isn’t “new”.
-	Explain core prediction tools already familiar in science (logistic regression, K-means) and where simple neural-network models extend them.
-	Discuss the assumptions that are incorporated into these models at each step in the process and how they can impact the output.
-	Describe loss functions, gradient descent and the danger of over-fitting in plain language. 

2. Day 2 - How LLMs Work
- Connect “Generative – Pre-trained – Transformer” (GPT) terminology to yesterday’s concepts.
- Demonstrate tokenization & embeddings through live games (Semantris).and demos
- Identify strengths & limits of large models in different modalities (text vs. code vs. images).

3. Day 3 – Prompt Engineering, Ethics, Alignment & Looking Ahead
- Identify ethical frameworks (Belmont, bias, alignment risks) and articulate why evaluation matters.
- Practice zero-, few-shot prompting; iterate toward higher-quality outputs.
- Evaluate model answers (accuracy, bias, toxicity) and understand alignment debates.
- Envision productive, ethical uses of LLMs in coding, research and daily tasks

4. Day 4 – Inside Neural Networks (FC, CNN, RNN)
- Describe a perceptron and connect it back to the logistic regression model.
- Explain activation functions and why multiple neurons form fully connected layers.
- Differentiate CNNs, RNNs and when to favour each architecture.
- Recognize limitations of each (e.g., vanishing gradients, locality bias). 

5. Day 5 – Generative Models & Transformers
- Contrast generative vs. discriminative / predictive models; outline GAN generator–discriminator interplay.
- Explain at a high level how self-attention works and why it replaced recurrence for many tasks.
- Appreciate the power (and pitfalls) of large-scale pre-training through guest talk & games.

## Lecture notes (pdfs)
- [Module 1](Content/AiOverview_Module1_Dec2025.pdf)
- [Module 2_and_3](Content/LLM_Overview_Module2_Module3_dec2025.pdf)
- [Module 4](Content/Module_4_Intro_to_Neural_Networks.pdf)
- [Module 5](Content/Module_5_Generative_Models_Transformers.pdf)  

